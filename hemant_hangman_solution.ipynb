{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhM3+EEV3meMYRcPhM5d97",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heyymant/Trexquant-Hangman/blob/main/hemant_hangman_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hangman API Solution Methodology\n",
        "by **Hemant Kumar** \\\n",
        "[Linkedin](linkedin.com/in/hemant-kumar-174157193/)\\\n",
        "[Github](https://huggingface.co/docs/transformers/main/en/model_doc/canine#transformers.CanineModel)\n",
        "#### Overview\n",
        "The solution to the Hangman game problem involves the development of a predictive model integrated into an API-based system. The primary goal is to efficiently guess letters in a partially known word, leveraging a set of helper functions and an underlying model to predict the next best guess. The approach balances both heuristic methods (fallback strategies) and model predictions to improve the guess accuracy, achieving an approximate 58.7% accuracy rate.\n",
        "\n",
        "The architecture is designed to utilize:\n",
        "1. **Helper Functions** – to process and manipulate word data and make decisions on which letters to guess.\n",
        "2. **Model Prediction** – which uses data-driven insights to guess the most likely next letter.\n",
        "3. **Game Interaction** – which integrates with the Hangman API to initiate, play, and track the game state.\n",
        "\n",
        "#### Overview of the CANINE Model\n",
        "The CANINE model is a transformer-based architecture that stands out from other models like BERT and RoBERTa due to its tokenization-free design. Unlike traditional models that rely on tokenizers such as WordPiece or SentencePiece, CANINE operates directly on character-level inputs, where each character is converted into its Unicode code point. This eliminates the need for complex tokenization processes and allows the model to work efficiently with various languages and word structures.\n",
        "\n",
        "The CANINE model was pretrained on a large multilingual corpus (104 languages) and uses Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) objectives for self-supervised learning. It captures semantic and syntactic relationships within text and can be fine-tuned for specific tasks such as word completion, making it ideal for the Hangman game.\n",
        "\n",
        "[Cilck to learn more about the Google Canine Model](https://huggingface.co/google/canine-s)\n",
        "\n",
        "[Google Canine Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/canine#transformers.CanineModel)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### Solution Methodology\n",
        "\n",
        "##### 1. **Game Setup & Initialization**\n",
        "The game begins by establishing a connection to the Hangman API using an `access_token` and a session. The Hangman API endpoint is dynamically selected based on response times from different servers (using the `determine_hangman_url()` method). Upon starting a game, the initial word is retrieved, and the game status is tracked as the guesses are made.\n",
        "\n",
        "##### 2. **Helper Functions: Roles and Importance**\n",
        "Several helper functions play a pivotal role in shaping the guessing strategy and optimizing the accuracy. These functions are responsible for processing the available word data, narrowing down potential matches, and guiding the model in making more accurate predictions. Below are the key functions:\n",
        "\n",
        "##### 2.1 **`build_dictionary()`**\n",
        "This function reads from a file containing a large dictionary of words, enabling the system to generate a comprehensive list of potential words to match against the current game state. This foundational word list helps in creating possible word patterns.\n",
        "\n",
        "##### 2.2 **`build_n_word_dictionary()`**\n",
        "This function organizes the full dictionary of words by length, making it easier to filter words of a specific length. The dictionary is divided into groups based on word length, allowing the system to focus on a relevant subset of words when attempting to guess the word.\n",
        "\n",
        "##### 2.3 **`vowel_count()`**\n",
        "The function calculates the proportion of vowels in a given word. This is used to adjust the guessing strategy based on the vowel density of the word. For example, if the word contains many vowels, the system may prioritize guessing vowels and vice versa. The ability to adapt based on vowel proportion helps refine the predictions.\n",
        "\n",
        "##### 2.4 **`func()` and `func2()`**\n",
        "These functions are used to process the word lists and count the frequency of individual letters within those words. `func()` generates a frequency count of letters across all words, while `func2()` narrows down the word list based on a specific word pattern. These functions are crucial in identifying which letters are most likely to appear in a word, thereby improving the guessing accuracy.\n",
        "\n",
        "##### 2.5 **`guess()` and `fallback_guess()`**\n",
        "These functions integrate the entire decision-making process. The `guess()` function leverages model predictions to guess the next most likely letter. If no prediction is made by the model (i.e., `None`), the `fallback_guess()` function (55.6% accuracy) uses a series of heuristics to choose a letter from the possible candidates. These heuristics include:\n",
        "- Prioritizing commonly used letters.\n",
        "- Filtering by word length and matching patterns.\n",
        "- Prioritizing vowels if the word appears to have a high vowel proportion.\n",
        "\n",
        "##### 2.6 **`request()`**\n",
        "This function manages communication with the Hangman API to request new game sessions, send guesses, and retrieve updated game states. It is integral to managing the game's lifecycle and tracking the results of each guess.\n",
        "\n",
        "##### 3. **Model Prediction and Guessing Strategy**\n",
        "The `guess()` function works by first attempting to get a prediction from the model. This model uses the `encoded_word` (which is the current game state with masked letters) along with previously guessed letters to predict the next most likely letter. If the model cannot make a prediction, it falls back on a heuristic strategy that leverages the word pattern, common letter frequencies, and vowel-consonant distribution.\n",
        "\n",
        "- **Model-Based Guessing:** The primary method relies on predicting the next letter based on the encoded word and previously guessed letters. This method assumes that the model has been trained or configured to understand letter frequencies, word structures, and common linguistic patterns.\n",
        "  \n",
        "- **Fallback Strategy:** If the model prediction is uncertain, the fallback strategy comes into play. This involves filtering the dictionary by word length, applying regular expression matching for word patterns, and considering the distribution of vowels and consonants. By refining the pool of potential words, the system can make a better guess on the next letter.\n",
        "\n",
        "**Note: The solution and model performs relatively better for long letter (length >=7)**\n",
        "\n",
        "##### 4. **Accuracy & Performance**\n",
        "The combination of the model-based approach and fallback strategy leads to a significant improvement in the game's performance. By using:\n",
        "- A large dictionary of words.\n",
        "- Frequency analysis of letter occurrences.\n",
        "- Pattern matching and vowel-consonant balancing.\n",
        "\n",
        "The solution achieves a 58.7% accuracy rate, which is a substantial improvement over random guessing. The accuracy is further bolstered by the ability of the system to adapt to the structure of the word as it is progressively revealed during gameplay.\n",
        "\n",
        "#### Importance of Helper Functions\n",
        "Each helper function plays a critical role in processing data, reducing complexity, and improving the accuracy of the solution. Here's why they are important:\n",
        "- **Data organization**: Functions like `build_dictionary()` and `build_n_word_dictionary()` organize data into a format that the model can easily access, making it more efficient to process.\n",
        "- **Frequency analysis**: Functions like `func()` and `func2()` allow the system to prioritize guesses based on letter frequency, which is essential for making smart guesses that have a higher probability of success.\n",
        "- **Pattern matching**: The `vowel_count()` function and other heuristic methods ensure that the system dynamically adjusts its strategy based on the characteristics of the word.\n",
        "- **Error handling**: Functions like `request()` handle API communication and error management, ensuring that the system operates smoothly despite potential network or game-specific errors.\n",
        "\n",
        "In conclusion, the combination of a structured approach to data handling, frequency analysis, and fallback strategies, powered by well-designed helper functions, leads to a robust and efficient Hangman game solver. The 58.7% accuracy reflects the balance of model-based predictions and heuristic guesswork, making it a highly effective solution in a game of Hangman.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "56NRYZT3G3rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **CANINE model** was an excellent choice for the Hangman game solution for several key reasons:\n",
        "\n",
        "### 1. **Character-Level Processing**\n",
        "   - **CANINE operates at the character level**, unlike many models that rely on tokenized words or subwords (such as WordPiece or SentencePiece). This characteristic is particularly advantageous for the Hangman game, where words are incomplete, and individual letters need to be predicted based on limited context.\n",
        "   - In Hangman, a word is progressively revealed, and the model must infer missing letters. CANINE's ability to process text directly as sequences of characters makes it ideal for understanding incomplete word structures and predicting the most probable missing letters.\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Multilingual Pretraining**\n",
        "   - **CANINE was pretrained on a large multilingual corpus**, including 104 languages, making it adaptable to a variety of word structures and languages. While the Hangman game might not require multilingual capabilities, the model's broad linguistic knowledge allows it to generalize better to different word patterns, increasing its flexibility and making it effective across a wide range of game scenarios.\n",
        "   - Additionally, this multilingual pretraining makes **CANINE more resilient** to variations in spelling and language, which could be useful if the game expands to different languages in the future.\n",
        "\n",
        "### 3. **Masked Language Modeling (MLM)**\n",
        "   - During **pretraining**, CANINE was trained using **Masked Language Modeling (MLM)**, where parts of the input text were masked, and the model had to predict the missing pieces. This pretraining task mirrors the core of the Hangman game, where part of a word is hidden, and the model needs to predict the missing letters.\n",
        "   - The MLM objective made CANINE well-suited to predict the missing letters in a partially revealed word. The model was trained to understand context and predict the most likely characters to fill in blanks, which directly aligns with the task of guessing letters in Hangman.\n",
        "\n",
        "### 4. **Handling Incomplete Data**\n",
        "   - Hangman presents a unique challenge: the model must make predictions based on incomplete information. **CANINE's training on masked language tasks** (such as MLM) made it particularly adept at handling incomplete or partially observed data. This gave the model an inherent advantage when tasked with predicting missing letters in the Hangman game, as it is already trained to fill in gaps in text.\n",
        "\n",
        "\n",
        "### 5. **Adaptability to Downstream Tasks**\n",
        "   - While CANINE was pretrained on a variety of tasks, it was designed to be **fine-tuned** for specific downstream applications. The ability to fine-tune the model on Hangman-specific data allowed it to specialize and adapt its predictions, improving the accuracy of letter guesses over time. Fine-tuning CANINE for Hangman led to a **58.7% accuracy**, demonstrating its ability to perform well on the task with the right adjustments.\n",
        "\n"
      ],
      "metadata": {
        "id": "knpszlXgI7KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7V8Tr_-tTWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}